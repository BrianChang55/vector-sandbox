# Phase 2 Plan 1: MainAgentService Core

**Goal**: Create the MainAgentService orchestrator with state machine pattern for questioning flow

## Context

From 01-03-SUMMARY.md:
- QuestioningService is now pure distillation (extracts facts, no decisions)
- Available methods: `detect_skip_request()`, `generate_question()`, `extract_facts()`
- Extraction output: `{goals, explicit_requirements, ui_mentions, unknowns}`

From 02-CONTEXT.md:
- State machine: idle → (questioning OR skip) → extracted → ready
- Control-flow decisions, not content decisions
- Use QuestioningSession model for persistence
- Parallel to existing flow (new endpoint later)

## Tasks

### Task 1: Create MainAgentService with state machine

Create `vector_app/services/main_agent_service.py` with:

```python
"""
Main Agent Service - Central Orchestrator

Makes all control-flow decisions for the questioning phase.
Uses QuestioningService for distillation (extracting facts, generating questions).

ARCHITECTURAL PRINCIPLE: This service DECIDES, subservices DISTILL.
- QuestioningService extracts facts from conversation
- MainAgentService decides: ask more? enough? skip?

State machine: idle → (questioning OR skip) → extracted → ready
"""

from dataclasses import dataclass
from enum import StrEnum
from typing import Any, Dict, List, Optional, TYPE_CHECKING

from vector_app.ai.client import get_llm_client
from vector_app.ai.models import AIModel
from vector_app.ai.types import LLMSettings
from vector_app.services.questioning_service import (
    QuestioningService,
    get_questioning_service,
)

if TYPE_CHECKING:
    from vector_app.models import QuestioningSession


class AgentState(StrEnum):
    """State machine states for the questioning flow."""
    IDLE = "idle"
    QUESTIONING = "questioning"
    SKIP = "skip"
    EXTRACTED = "extracted"
    READY = "ready"


@dataclass
class AgentDecision:
    """Result of an agent decision.

    Attributes:
        action: What to do next (ask_question, extract, proceed)
        next_state: State to transition to
        question: The question to ask (if action is ask_question)
        extracted_facts: Facts extracted (if action is extract)
        reasoning: Why this decision was made
    """
    action: str  # "ask_question" | "extract" | "proceed" | "skip"
    next_state: AgentState
    question: Optional[str] = None
    extracted_facts: Optional[Dict[str, Any]] = None
    reasoning: str = ""


class MainAgentService:
    """
    Central orchestrator for the questioning flow.

    Makes control-flow decisions:
    - Should I ask another question?
    - Is the last answer vague or clear?
    - Did the user request to skip?
    - Do I have enough information to proceed?

    These are CONTROL-FLOW decisions, not CONTENT decisions.
    The QuestioningService provides the raw facts; this service decides what to do.
    """

    def __init__(self):
        self._questioning = get_questioning_service()

    def process_user_message(
        self,
        session: "QuestioningSession",
        user_message: str,
        chat_history: List[Dict],
    ) -> AgentDecision:
        """Process a user message and decide what to do next.

        This is the main entry point. Given a user message:
        1. Check if user requested to skip
        2. If not skipping, decide if we need more questions
        3. Return the appropriate action

        Args:
            session: The QuestioningSession being processed
            user_message: The latest user message
            chat_history: Full conversation history

        Returns:
            AgentDecision with action, next_state, and optional data
        """
        # Check for skip request first
        skip_requested, skip_keyword = self._questioning.detect_skip_request(user_message)
        if skip_requested:
            return self._handle_skip(session, chat_history, skip_keyword)

        # Decide if we need more questions or have enough
        return self._decide_next_step(session, chat_history)

    def _handle_skip(
        self,
        session: "QuestioningSession",
        chat_history: List[Dict],
        skip_keyword: str,
    ) -> AgentDecision:
        """Handle user skip request — extract what we have and proceed."""
        # Extract whatever facts we have
        facts = self._questioning.extract_facts(
            chat_history=chat_history,
            initial_request=session.initial_request,
        )

        return AgentDecision(
            action="skip",
            next_state=AgentState.READY,
            extracted_facts=facts,
            reasoning=f"User requested skip with keyword: '{skip_keyword}'",
        )

    def _decide_next_step(
        self,
        session: "QuestioningSession",
        chat_history: List[Dict],
    ) -> AgentDecision:
        """Decide whether to ask more questions or proceed.

        Uses LLM to make control-flow decision based on:
        - Chat history and what's been answered
        - Number of questions already asked
        - Clarity of recent answers
        """
        # First extract current facts to inform decision
        facts = self._questioning.extract_facts(
            chat_history=chat_history,
            initial_request=session.initial_request,
        )

        # Use LLM to decide: ask more or proceed?
        should_continue = self._should_continue_questioning(
            session=session,
            chat_history=chat_history,
            extracted_facts=facts,
        )

        if should_continue:
            # Generate next question
            question = self._questioning.generate_question(
                chat_history=chat_history,
                initial_request=session.initial_request,
                question_count=session.question_count,
            )

            if question:
                return AgentDecision(
                    action="ask_question",
                    next_state=AgentState.QUESTIONING,
                    question=question,
                    reasoning="More clarification needed",
                )

        # We have enough — extract and proceed
        return AgentDecision(
            action="proceed",
            next_state=AgentState.READY,
            extracted_facts=facts,
            reasoning="Sufficient information gathered",
        )

    def _should_continue_questioning(
        self,
        session: "QuestioningSession",
        chat_history: List[Dict],
        extracted_facts: Optional[Dict[str, Any]],
    ) -> bool:
        """LLM-based decision: should we ask more questions?

        Considers:
        - How many unknowns remain?
        - Are answers clear or vague?
        - How many questions have we asked?
        - Is the request simple enough to proceed?
        """
        # TODO: Implement LLM-based sufficiency check
        # For now, simple heuristic: max 5 questions
        if session.question_count >= 5:
            return False

        # Check if we have critical unknowns
        if extracted_facts:
            unknowns = extracted_facts.get("unknowns", [])
            # If many unknowns and we haven't asked much, continue
            if len(unknowns) > 2 and session.question_count < 3:
                return True

        return False


# Singleton instance
_main_agent_service: Optional[MainAgentService] = None


def get_main_agent_service() -> MainAgentService:
    """Get singleton main agent service instance."""
    global _main_agent_service
    if _main_agent_service is None:
        _main_agent_service = MainAgentService()
    return _main_agent_service
```

**Files:**
- Create: `vector_app/services/main_agent_service.py`

**Verify:**
```bash
source venv/bin/activate && python -c "
import os
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'internal_apps.settings')
import django
django.setup()
from vector_app.services.main_agent_service import (
    MainAgentService, get_main_agent_service, AgentState, AgentDecision
)
print('MainAgentService imported successfully')
print('States:', list(AgentState))
"
```

### Task 2: Add sufficiency decision prompt

Create the LLM prompt for sufficiency decisions in `vector_app/prompts/main_agent.py`:

```python
"""
Main Agent Prompts

Prompts for the MainAgentService control-flow decisions.
These prompts help the agent decide WHEN to act, not WHAT to build.
"""

SUFFICIENCY_DECISION_SYSTEM_PROMPT = """You are an assistant that decides if enough information has been gathered to build an app.

You will be given:
- The user's initial request
- The conversation so far
- Facts extracted from the conversation (goals, requirements, UI mentions, unknowns)
- Number of questions already asked

Your job is to decide: Should we ask more clarifying questions, or do we have enough to proceed?

DECISION CRITERIA:
- If there are critical unknowns (core functionality unclear), ask more
- If answers have been vague/unclear, ask for clarification
- If we've asked 5+ questions, likely enough — proceed
- If the request is simple and clear, proceed even with 0 questions
- If core goals and requirements are understood, proceed

Respond with JSON:
{
  "should_continue": true/false,
  "reasoning": "Brief explanation of why",
  "critical_unknowns": ["list of unknowns that would block building"]
}"""


def build_sufficiency_decision_prompt(
    initial_request: str,
    chat_history: str,
    extracted_facts: dict,
    question_count: int,
) -> str:
    """Build prompt for sufficiency decision.

    Args:
        initial_request: The original user request
        chat_history: Formatted conversation history
        extracted_facts: Facts extracted by QuestioningService
        question_count: Number of questions asked so far

    Returns:
        Formatted prompt string
    """
    facts_str = _format_facts(extracted_facts) if extracted_facts else "No facts extracted yet."

    return f"""## Initial Request
{initial_request}

## Conversation So Far
{chat_history}

## Extracted Facts
{facts_str}

## Questions Asked
{question_count} questions have been asked.

## Your Decision
Should we ask more clarifying questions, or proceed with building?"""


def _format_facts(facts: dict) -> str:
    """Format extracted facts for prompt."""
    lines = []

    if goals := facts.get("goals"):
        lines.append("**Goals:**")
        for g in goals:
            lines.append(f"- {g}")

    if reqs := facts.get("explicit_requirements"):
        lines.append("\n**Requirements:**")
        for r in reqs:
            lines.append(f"- {r}")

    if ui := facts.get("ui_mentions"):
        lines.append("\n**UI/UX:**")
        for u in ui:
            lines.append(f"- {u}")

    if unknowns := facts.get("unknowns"):
        lines.append("\n**Unknowns:**")
        for u in unknowns:
            lines.append(f"- {u}")

    return "\n".join(lines) if lines else "No facts extracted."
```

**Files:**
- Create: `vector_app/prompts/main_agent.py`

**Verify:**
```bash
source venv/bin/activate && python -c "
import os
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'internal_apps.settings')
import django
django.setup()
from vector_app.prompts.main_agent import (
    SUFFICIENCY_DECISION_SYSTEM_PROMPT,
    build_sufficiency_decision_prompt,
)
print('Prompts imported successfully')
print('System prompt length:', len(SUFFICIENCY_DECISION_SYSTEM_PROMPT))
"
```

### Task 3: Implement LLM-based sufficiency check

Update `MainAgentService._should_continue_questioning()` to use the LLM prompt:

```python
def _should_continue_questioning(
    self,
    session: "QuestioningSession",
    chat_history: List[Dict],
    extracted_facts: Optional[Dict[str, Any]],
) -> bool:
    """LLM-based decision: should we ask more questions?"""
    from vector_app.prompts.main_agent import (
        SUFFICIENCY_DECISION_SYSTEM_PROMPT,
        build_sufficiency_decision_prompt,
    )

    # Hard limit: don't exceed max questions
    MAX_QUESTIONS = 5
    if session.question_count >= MAX_QUESTIONS:
        return False

    # Format chat history for prompt
    formatted_history = self._format_chat_history(chat_history)

    prompt = build_sufficiency_decision_prompt(
        initial_request=session.initial_request,
        chat_history=formatted_history,
        extracted_facts=extracted_facts or {},
        question_count=session.question_count,
    )

    try:
        result = get_llm_client().run(
            system_prompt=SUFFICIENCY_DECISION_SYSTEM_PROMPT,
            user_prompt=prompt,
            llm_settings=LLMSettings(
                model=AIModel.CLAUDE_HAIKU_4_5,  # Fast model for control decisions
                temperature=0.1,
                max_tokens=500,
                timeout=15.0,
            ),
            json_mode=True,
        )

        import json
        data = result.validated(json.loads, default=None)
        if data and isinstance(data, dict):
            return data.get("should_continue", False)

        return False

    except Exception as e:
        logger.warning("Sufficiency check failed: %s, defaulting to proceed", e)
        return False

def _format_chat_history(self, chat_history: List[Dict]) -> str:
    """Format chat history for prompt."""
    if not chat_history:
        return "No conversation yet."

    lines = []
    for msg in chat_history:
        role = msg.get("role", "user").capitalize()
        content = msg.get("content", "")
        lines.append(f"{role}: {content}")

    return "\n".join(lines)
```

**Files:**
- Modify: `vector_app/services/main_agent_service.py`

**Verify:**
```bash
source venv/bin/activate && python -c "
import os
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'internal_apps.settings')
import django
django.setup()
from vector_app.services.main_agent_service import get_main_agent_service
svc = get_main_agent_service()
# Verify method exists and has LLM import
import inspect
src = inspect.getsource(svc._should_continue_questioning)
assert 'SUFFICIENCY_DECISION_SYSTEM_PROMPT' in src
print('LLM-based sufficiency check implemented')
"
```

## Commit Strategy

Each task should be committed separately:
- Task 1: `feat(02-01): create MainAgentService with state machine pattern`
- Task 2: `feat(02-01): add sufficiency decision prompts`
- Task 3: `feat(02-01): implement LLM-based sufficiency check`

## Success Criteria

- [ ] MainAgentService class exists with state machine pattern
- [ ] AgentState enum has all states: idle, questioning, skip, extracted, ready
- [ ] AgentDecision dataclass captures action, state, question, facts
- [ ] `process_user_message()` handles skip detection and decision flow
- [ ] LLM-based `_should_continue_questioning()` uses prompt for decision
- [ ] Singleton pattern matches existing services (IntentClassifier, QuestioningService)

---
*Phase: 02-main-agent-service*
*Plan: 01*
