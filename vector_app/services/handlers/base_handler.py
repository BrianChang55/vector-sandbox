"""
Base Handler

Abstract base class for all intent handlers.
Provides common utilities for AgentEvent generation and LLM interactions.
"""

import json
import logging
import re
import time
import uuid
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
from typing import Any, Dict, Generator, List, Optional, Set, TYPE_CHECKING

from django.conf import settings
import httpx

from vector_app.services.types import (
    FileChange,
    CompilationError,
    ValidationResult,
    AgentEvent,
)
from vector_app.services.validation_service import get_validation_service

if TYPE_CHECKING:
    from vector_app.models import InternalApp, AppVersion
    from vector_app.services.intent_classifier import IntentResult
    from vector_app.services.context_analyzer import AppContext

logger = logging.getLogger(__name__)



def exclude_protected_files(
    files: List[FileChange],
    protected: Set[str],
) -> List[FileChange]:
    """
    Filter out protected files that should not be overwritten by LLM output.
    
    These files are auto-generated by the system (e.g., TypeScript types from
    table schemas) and should not be modified by the code generation agent.
    
    Args:
        files: List of FileChange objects to filter
        protected: Set of file paths that are protected (defaults to PROTECTED_FILES)
    
    Returns:
        Filtered list with protected files removed
    """
    filtered = [
        f for f in files if f.path not in protected
    ]
    skipped_protected = [
        f for f in files if f.path in protected
    ]
    if skipped_protected:
        logger.warning(
            f"[PROTECTED] Blocked LLM from overwriting protected file: {', '.join(skipped_protected)}"
        )

    return filtered


@dataclass
class PlanStep:
    """A single step in the execution plan."""

    id: str
    type: str
    title: str
    description: str
    step_order: int = 0  # Wave number for parallel execution (0 = first)
    status: str = "pending"
    duration: Optional[int] = None
    output: Optional[str] = None


class StreamingValidator:
    """
    Validates code during streaming to catch issues early.

    Performs lightweight checks on accumulated content to detect:
    - Excessive 'any' type usage
    - Debug console.log statements
    - TODO/FIXME placeholders
    - Bracket imbalance
    - Empty catch blocks
    """

    def __init__(self):
        self.warnings: List[str] = []
        self.any_count = 0
        self.console_log_count = 0
        self.todo_count = 0
        self._last_check_length = 0

    def check_chunk(self, chunk: str, accumulated: str) -> List[str]:
        """
        Check a new chunk for issues.

        Only performs expensive checks periodically to avoid overhead.

        Returns list of new warnings (if any).
        """
        new_warnings = []

        # Quick checks on the new chunk
        if "as any" in chunk:
            self.any_count += chunk.count("as any")
            if self.any_count > 3:
                warning = f"Excessive 'as any' usage ({self.any_count}x) - consider proper types"
                if warning not in self.warnings:
                    self.warnings.append(warning)
                    new_warnings.append(warning)

        if "console.log" in chunk:
            self.console_log_count += chunk.count("console.log")
            if self.console_log_count > 2:
                warning = (
                    f"Debug console.log statements ({self.console_log_count}x) - remove before production"
                )
                if warning not in self.warnings:
                    self.warnings.append(warning)
                    new_warnings.append(warning)

        if "// TODO" in chunk or "// FIXME" in chunk:
            self.todo_count += 1
            warning = "TODO/FIXME placeholder detected - ensure completion"
            if warning not in self.warnings:
                self.warnings.append(warning)
                new_warnings.append(warning)

        # Periodic deeper checks (every 2000 chars)
        if len(accumulated) - self._last_check_length > 2000:
            self._last_check_length = len(accumulated)

            # Check for empty catch blocks
            if "catch" in accumulated and "catch (e) {}" in accumulated.replace(" ", ""):
                warning = "Empty catch block detected - add error handling"
                if warning not in self.warnings:
                    self.warnings.append(warning)
                    new_warnings.append(warning)

        return new_warnings

    def final_check(self, content: str) -> List[str]:
        """
        Perform final validation checks on complete content.

        Returns list of final warnings.
        """
        final_warnings = []

        # Check bracket balance
        open_braces = content.count("{")
        close_braces = content.count("}")
        if abs(open_braces - close_braces) > 2:
            final_warnings.append(f"Bracket imbalance: {open_braces} open vs {close_braces} close")

        open_parens = content.count("(")
        close_parens = content.count(")")
        if abs(open_parens - close_parens) > 2:
            final_warnings.append(f"Parenthesis imbalance: {open_parens} open vs {close_parens} close")

        # Check for incomplete code patterns
        if "..." in content and "rest" not in content.lower():
            final_warnings.append("Ellipsis '...' found - may indicate incomplete code")

        return final_warnings

    def get_all_warnings(self) -> List[str]:
        """Get all accumulated warnings."""
        return self.warnings.copy()


class BaseHandler(ABC):
    """
    Abstract base class for intent handlers.

    All handlers must implement the execute() method which yields AgentEvent
    objects for the streaming response.
    """

    OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"

    def __init__(self):
        self.api_key = getattr(settings, "OPENROUTER_API_KEY", None) or getattr(
            settings, "OPENAI_API_KEY", None
        )
        self.app_name = getattr(settings, "OPENROUTER_APP_NAME", "Internal Apps Builder")
        self.site_url = getattr(settings, "BASE_URL", "http://localhost:8001")

    def _build_headers(self) -> Dict[str, str]:
        """Build API headers for OpenRouter."""
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": self.site_url,
            "X-Title": self.app_name,
        }

    @abstractmethod
    def execute(
        self,
        intent: "IntentResult",
        context: "AppContext",
        user_message: str,
        current_spec: Optional[Dict[str, Any]],
        registry_surface: Dict[str, Any],
        app_name: str,
        model: str,
        app: Optional["InternalApp"] = None,
        version: Optional["AppVersion"] = None,
        **kwargs,
    ) -> Generator[AgentEvent, None, List[FileChange]]:
        """
        Execute the handler for the given intent.

        Args:
            intent: Classified intent result
            context: App context from analyzer
            user_message: Original user request
            current_spec: Current app specification
            registry_surface: Available data resources
            app_name: Name of the app
            model: LLM model to use
            app: Optional InternalApp instance
            version: Optional AppVersion instance

        Yields:
            AgentEvent objects for streaming

        Returns:
            List of FileChange objects representing all changes
        """
        pass

    # ===== Common Event Emitters =====

    def emit_phase_change(self, phase: str, message: str) -> AgentEvent:
        """Emit a phase change event."""
        return AgentEvent(
            "phase_change",
            {
                "phase": phase,
                "message": message,
            },
        )

    def emit_thinking(self, content: str, thinking_type: str = "reasoning") -> AgentEvent:
        """Emit a thinking event."""
        return AgentEvent(
            "thinking",
            {
                "content": content,
                "type": thinking_type,
            },
        )

    def emit_plan_created(
        self,
        steps: List[PlanStep],
        explored_dirs: int = 0,
        explored_files: int = 0,
        searches: int = 0,
    ) -> AgentEvent:
        """Emit a plan created event."""
        plan = {
            "id": str(uuid.uuid4()),
            "goal": "Execute plan",
            "reasoning": "Plan created based on user intent",
            "steps": [asdict(s) for s in steps],
            "estimated_duration": len(steps) * 5000,
        }
        return AgentEvent(
            "plan_created",
            {
                "plan": plan,
                "steps": [asdict(s) for s in steps],
                "exploredDirectories": explored_dirs,
                "exploredFiles": explored_files,
                "searches": searches,
            },
        )

    def emit_step_started(self, step: PlanStep, step_index: int) -> AgentEvent:
        """Emit step started event."""
        return AgentEvent(
            "step_started",
            {
                "stepId": step.id,
                "stepIndex": step_index,
            },
        )

    def emit_step_start(self, step: PlanStep, step_index: int) -> AgentEvent:
        """Emit step start event (legacy format)."""
        return AgentEvent(
            "step_start",
            {
                "step_index": step_index,
                "step": asdict(step),
            },
        )

    def emit_step_completed(self, step: PlanStep, step_index: int, duration: int) -> AgentEvent:
        """Emit step completed event."""
        return AgentEvent(
            "step_completed",
            {
                "stepId": step.id,
                "stepIndex": step_index,
                "duration": duration,
            },
        )

    def emit_step_complete(self, step_index: int, status: str, duration: int) -> AgentEvent:
        """Emit step complete event (legacy format)."""
        return AgentEvent(
            "step_complete",
            {
                "step_index": step_index,
                "status": status,
                "duration": duration,
            },
        )

    def emit_step_progress(self, step_index: int, progress: int, message: str) -> AgentEvent:
        """Emit step progress event."""
        return AgentEvent(
            "step_progress",
            {
                "step_index": step_index,
                "progress": min(100, progress),
                "message": message,
            },
        )

    def emit_file_generated(self, file: FileChange) -> AgentEvent:
        """Emit file generated event."""
        return AgentEvent(
            "file_generated",
            {
                "file": file.to_dict(),
            },
        )

    def emit_table_created(self, slug: str, name: str, columns: int) -> AgentEvent:
        """Emit table created event."""
        return AgentEvent(
            "table_created",
            {
                "slug": slug,
                "name": name,
                "columns": columns,
            },
        )

    def emit_table_updated(
        self,
        slug: str,
        name: str,
        added: List[str] = None,
        removed: List[str] = None,
        modified: List[str] = None,
    ) -> AgentEvent:
        """Emit table updated event."""
        return AgentEvent(
            "table_updated",
            {
                "slug": slug,
                "name": name,
                "changes": {
                    "added": added or [],
                    "removed": removed or [],
                    "modified": modified or [],
                },
            },
        )

    def emit_validation_result(
        self,
        passed: bool,
        errors: List[Dict[str, Any]] = None,
        warnings: List[str] = None,
        fix_attempts: int = 0,
    ) -> AgentEvent:
        """Emit validation result event."""
        return AgentEvent(
            "validation_result",
            {
                "passed": passed,
                "errors": errors or [],
                "warnings": warnings or [],
                "fix_attempts": fix_attempts,
            },
        )

    # ===== Streaming Validation =====

    def create_streaming_validator(self) -> StreamingValidator:
        """Create a new streaming validator instance."""
        return StreamingValidator()

    def emit_streaming_warning(self, warning: str) -> AgentEvent:
        """Emit a warning detected during streaming."""
        return AgentEvent(
            "streaming_warning",
            {
                "warning": warning,
                "severity": "low",
            },
        )

    # ===== LLM Utilities =====

    def stream_llm_response(
        self,
        system_prompt: str,
        user_prompt: str,
        model: str,
        temperature: float = 0.3,
        timeout: float = 120.0,
    ) -> Generator[str, None, str]:
        """
        Stream LLM response and yield chunks.

        Yields individual content chunks as they arrive.
        Returns the full accumulated content.
        """
        full_content = ""

        try:
            with httpx.Client(timeout=timeout) as client:
                with client.stream(
                    "POST",
                    self.OPENROUTER_API_URL,
                    headers=self._build_headers(),
                    json={
                        "model": model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        "temperature": temperature,
                        "stream": True,
                    },
                ) as response:
                    response.raise_for_status()

                    for line in response.iter_lines():
                        if not line:
                            continue

                        if line.startswith("data: "):
                            data = line[6:]
                            if data == "[DONE]":
                                break

                            try:
                                chunk_data = json.loads(data)
                                delta = chunk_data.get("choices", [{}])[0].get("delta", {})
                                content = delta.get("content", "")

                                if content:
                                    full_content += content
                                    yield content

                            except json.JSONDecodeError:
                                continue

        except Exception as e:
            logger.error(f"LLM streaming error: {e}")
            raise

        return full_content

    def call_llm(
        self,
        system_prompt: str,
        user_prompt: str,
        model: str,
        temperature: float = 0.3,
        timeout: float = 60.0,
    ) -> str:
        """
        Make a non-streaming LLM call.

        Returns the complete response content.
        """
        try:
            with httpx.Client(timeout=timeout) as client:
                response = client.post(
                    self.OPENROUTER_API_URL,
                    headers=self._build_headers(),
                    json={
                        "model": model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        "temperature": temperature,
                    },
                )
                response.raise_for_status()

                result = response.json()
                return result["choices"][0]["message"]["content"]

        except Exception as e:
            logger.error(f"LLM call error: {e}")
            raise

    # ===== Code Parsing Utilities =====

    def parse_code_blocks(self, content: str) -> List[FileChange]:
        """
        Parse code blocks from LLM response into FileChange objects.

        Supports multiple formats:
        - ```filepath:path/to/file.ext
        - ```path/to/file.ext
        - // filepath: path/to/file.ext
        """
        files = []
        seen_paths = set()

        patterns = [
            # Pattern 1: ```filepath:path/to/file.ext (highest priority)
            r"```filepath:([^\n`]+)\n(.*?)```",
            # Pattern 2: ```src/path/to/file.ext (explicit src path)
            r"```(src/[^\n`]+\.[a-zA-Z]+)\n(.*?)```",
            # Pattern 3: ```path/to/file.ext (with extension, but not filepath:)
            r"```([a-zA-Z][^\n`]*\.[a-zA-Z]+)\n(.*?)```",
            # Pattern 4: // filepath: path/to/file.ext followed by code
            r"// filepath:\s*([^\n]+)\n(.*?)(?=// filepath:|```|$)",
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for filepath, code in matches:
                filepath = filepath.strip()
                code = code.strip()

                # Remove any filepath: prefix that might have been captured
                if filepath.startswith("filepath:"):
                    filepath = filepath[9:].strip()

                # Skip language-only markers
                if filepath.lower() in (
                    "tsx",
                    "ts",
                    "js",
                    "jsx",
                    "css",
                    "json",
                    "html",
                    "typescript",
                    "javascript",
                    "react",
                    "python",
                ):
                    continue

                # Skip if path contains 'filepath:' (malformed)
                if "filepath:" in filepath:
                    continue

                # Skip if no actual code
                if not code or len(code) < 10:
                    continue

                # Normalize the file path
                normalized_path = filepath
                if normalized_path.startswith("/"):
                    normalized_path = normalized_path[1:]
                if not normalized_path.startswith("src/"):
                    normalized_path = f"src/{normalized_path}"

                # Skip if we already have this file
                if normalized_path in seen_paths:
                    continue
                seen_paths.add(normalized_path)

                # Determine language from extension
                ext = normalized_path.split(".")[-1] if "." in normalized_path else "tsx"
                lang_map = {
                    "tsx": "tsx",
                    "ts": "ts",
                    "jsx": "tsx",
                    "js": "ts",
                    "css": "css",
                    "json": "json",
                    "html": "html",
                }

                # Clean up the code - remove trailing ```
                code = re.sub(r"```\s*$", "", code).strip()

                # For new files: lines_added = total lines, lines_removed = 0
                lines_in_code = code.count("\n") + (1 if code and not code.endswith("\n") else 0)

                files.append(
                    FileChange(
                        path=normalized_path,
                        action="create",
                        language=lang_map.get(ext, "tsx"),
                        content=code,
                        lines_added=lines_in_code,
                        lines_removed=0,
                    )
                )

        return files
    
    def create_step(self, step_type: str, title: str, description: str, step_order: int = 0) -> PlanStep:
        """Create a new plan step."""
        return PlanStep(
            id=str(uuid.uuid4()),
            type=step_type,
            title=title,
            description=description,
            step_order=step_order,
        )

    def get_language(self, path: str) -> str:
        """Determine language from file extension."""
        lang_map = {
            "tsx": "tsx",
            "ts": "ts",
            "jsx": "tsx",
            "js": "ts",
            "css": "css",
            "json": "json",
            "html": "html",
        }
        ext = path.split(".")[-1] if "." in path else "tsx"
        return lang_map.get(ext, "tsx")

    def log_llm_context(
        self,
        intent_name: str,
        handler_name: str,
        data_store_context: Optional[str] = None,
        mcp_tools_context: Optional[str] = None,
    ) -> None:
        """
        Debug log the context being passed to the LLM for each intent.
        
        Logs TypeScript types if present, otherwise logs all file context.
        """
        logger.debug(
            f"{'='*60}\n"
            f"[LLM CONTEXT DEBUG] Intent: {intent_name} | Handler: {handler_name}\n"
            f"[LLM CONTEXT DEBUG] Data Store Context: {data_store_context}\n"
            f"[LLM CONTEXT DEBUG] MCP Tools Context: {mcp_tools_context}\n"
            f"{'='*60}"
        )

    # ===== Validation Utilities =====

    def validate_and_fix(
        self,
        generated_files: List[FileChange],
        model: str,
        max_attempts: int = 2,
    ) -> Generator[AgentEvent, None, tuple]:
        """
        Validate generated TypeScript code and attempt to fix errors.

        Args:
            generated_files: List of files to validate
            model: LLM model for error fixing
            max_attempts: Maximum fix attempts

        Yields:
            AgentEvent objects for progress

        Returns:
            Tuple of (validation_passed, fix_attempts)
        """
        from vector_app.services.error_fix_service import get_error_fix_service

        validation_service = get_validation_service()
        validation_passed = False
        fix_attempts = 0

        for attempt in range(1, max_attempts + 1):
            # Run TypeScript validation
            validation = validation_service.validate_typescript(generated_files)

            if validation.passed:
                validation_passed = True
                break

            # Attempt to fix
            fix_attempts = attempt
            error_count = len(validation.errors)

            yield self.emit_thinking(
                f"Found {error_count} compilation error(s), attempting fix ({attempt}/{max_attempts})",
                "observation",
            )

            fix_service = get_error_fix_service()

            # Run fix service
            fixed_files_by_path = {}

            fix_gen = fix_service.fix_errors(
                files=generated_files,
                errors=validation.errors,
                model=model,
                attempt=attempt,
            )

            while True:
                try:
                    event = next(fix_gen)
                    yield event
                    if event.type == "file_generated":
                        file_data = event.data.get("file", {})
                        if file_data.get("path"):
                            fixed_files_by_path[file_data["path"]] = FileChange(
                                path=file_data["path"],
                                action=file_data.get("action", "modify"),
                                language=file_data.get("language", "tsx"),
                                content=file_data.get("content", ""),
                                previous_content=file_data.get("previous_content", ""),
                                lines_added=file_data.get("lines_added", 0),
                                lines_removed=file_data.get("lines_removed", 0),
                            )
                except StopIteration:
                    break

            # Apply fixed files
            if fixed_files_by_path:
                for i, f in enumerate(generated_files):
                    if f.path in fixed_files_by_path:
                        generated_files[i] = fixed_files_by_path[f.path]

        # Final validation check
        if not validation_passed:
            final = validation_service.validate_typescript(generated_files)
            validation_passed = final.passed

        return (validation_passed, fix_attempts)

    def _validate_datastore_field_names(
        self, files: List[FileChange], app: Optional["InternalApp"]
    ) -> Dict[str, Any]:
        """
        Validate that generated code uses exact field names from schemas.

        Orchestrates business logic by coordinating validation service methods.

        Returns:
            Dict with 'passed' (bool), 'errors' (list), 'warnings' (list)
        """
        if not app:
            return {"passed": True, "errors": [], "warnings": []}

        validation_service = get_validation_service()

        # Build table schemas
        table_schemas = validation_service.build_table_schemas(app)
        if not table_schemas:
            return {"passed": True, "errors": [], "warnings": []}

        logger.info(
            f"ðŸ” [FIELD VALIDATION] Validating {len(files)} files against {len(table_schemas)} table schemas"
        )

        # Collect all validation errors
        errors = []
        warnings = []

        # Validate table references exist
        errors.extend(validation_service.validate_table_references(files, table_schemas))

        # Validate field names in operations
        errors.extend(validation_service.validate_field_names_in_operations(files, table_schemas))

        # Validate row.id patterns
        errors.extend(validation_service.validate_row_id_patterns(files))

        # Report results
        passed = len(errors) == 0

        if errors:
            logger.error(f"ðŸš¨ [FIELD VALIDATION] FAILED: {len(errors)} field name errors found")
        elif warnings:
            logger.warning(f"âš ï¸ [FIELD VALIDATION] PASSED with {len(warnings)} warnings")
        else:
            logger.info(f"âœ… [FIELD VALIDATION] PASSED: All field names match schema")

        return {
            "passed": passed,
            "errors": errors,
            "warnings": warnings,
        }

    def _build_field_fix_prompt(
        self, original_content: str, errors: List[str], data_store_context: Optional[str] = None
    ) -> str:
        """Build a prompt to ask Claude to fix field name errors."""
        error_list = "\n".join(f"- {error}" for error in errors)

        # Check if any errors are about missing tables
        has_missing_tables = any(
            "non-existent table" in error or "not found" in error.lower() for error in errors
        )

        # Include data_store_context so Claude sees the current schema
        schema_section = ""
        if data_store_context:
            schema_section = f"\n\n**CURRENT DATABASE SCHEMA:**\n{data_store_context}\n"

        missing_table_instructions = ""
        if has_missing_tables:
            missing_table_instructions = """

**ðŸš¨ MISSING TABLES DETECTED:**
Some errors indicate you're trying to use tables that don't exist yet. To fix this:

1. **CREATE THE MISSING TABLES FIRST** using TABLE_DEFINITION blocks:
   ```table:table-slug
   name: Table Name
   description: What this table stores
   columns:
     - name: field_name, type: string, nullable: false
     - name: another_field, type: integer, default: 0
   ```

2. **DO NOT define reserved fields** (id, created_at, updated_at) - these are auto-generated
3. **THEN generate the code** that uses these tables

**IMPORTANT:** If you need to create tables, output the TABLE_DEFINITION blocks FIRST, then the code blocks.
"""

        return f"""Your previous code generation had field name validation errors. You used incorrect field names that don't match the database schema.

**ERRORS FOUND:**
{error_list}
{schema_section}
{missing_table_instructions}
**ðŸš¨ CRITICAL RULES TO FIX THESE ERRORS:**

1. **EACH TABLE HAS ITS OWN FIELDS** - You CANNOT use fields from one table when querying another!
   - Example: `sprint_number` belongs to the `sprints` table, NOT the `projects` table
   - You MUST check the schema above to see which fields belong to which table

2. **USE EXACT FIELD NAMES** - Do NOT rename or transform field names
   - If schema defines `title`, use `title` (not `taskTitle`, `task_title`, or `titleText`)
   - If schema defines `total_story_points`, use `total_story_points` (not `totalStoryPoints`)

3. **CHECK WHICH TABLE YOU'RE QUERYING** - Before using a field, verify it exists in THAT specific table
   - When calling `dataStore.query('projects', ...)`, you can ONLY use fields from the `projects` table
   - Look at the schema above - each table lists its available fields

**YOUR TASK:**
Please regenerate the ENTIRE code fixing ALL field name errors. Pay special attention to:
- Creating any missing tables FIRST (if errors mention "non-existent table") using TABLE_DEFINITION blocks
- Which table you're querying (the first parameter to dataStore.query/insert/update)
- Which fields are available in THAT specific table (check the schema above)
- Using exact field names as defined in the schema

**PREVIOUS CODE (WITH ERRORS):**
{original_content}

Generate the complete corrected code now."""
